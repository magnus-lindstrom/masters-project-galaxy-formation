{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))\n",
    "\n",
    "import os\n",
    "from os.path import expanduser\n",
    "home_dir = expanduser(\"~\")\n",
    "module_path = home_dir + '/code/modules/'\n",
    "models_path = home_dir + '/models/'\n",
    "import sys\n",
    "sys.path.append(module_path)\n",
    "fig_dir = 'figures/'\n",
    "bp_network_dir = home_dir + '/trained_networks/backprop_trained/'\n",
    "bp_pso_network_dir = home_dir + '/trained_networks/backprop_and_pso_trained/'\n",
    "import time\n",
    "import importlib\n",
    "from collections import Counter\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from keras import regularizers\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Input, Dense, LeakyReLU, concatenate\n",
    "from keras.callbacks import EarlyStopping, TerminateOnNaN\n",
    "from loading_datasets import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from scipy import stats\n",
    "#import model_management\n",
    "from scipy.special import comb\n",
    "import multiprocessing as mp\n",
    "import datetime\n",
    "import codecs, json\n",
    "import corner\n",
    "import pickle\n",
    "from itertools import combinations\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "%aimport data_processing\n",
    "%aimport multiprocessing_modules\n",
    "%aimport plotting\n",
    "%aimport model_setup\n",
    "from data_processing import *\n",
    "from multiprocessing_modules import train_net, init\n",
    "from plotting import *\n",
    "from model_setup import *\n",
    "\n",
    "np.random.seed(999)\n",
    "random.seed(999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Run on CPU only\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set parameter string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set name ending with parameters for figures to be saved\n",
    "param_string = 'nLayers_%d_nNeurons_%d_actFun_%s_lossFunc_%s_nTrainSamples_%d_nEpochs_%d_batchSize_%d' % (\n",
    "    nLayers, neuronsPerLayer, activationFunction, loss_function, train_size, nEpochs, batchSize)\n",
    "print(param_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get a feel for the data\n",
    "for i in range(len(input_features)):\n",
    "    print(input_features[i],': min: %.2e, max: %.2e.' % (np.min(x_train[:,i]), np.max(x_train[:,i])))\n",
    "for i in range(len(output_features)):\n",
    "    print(output_features[i],': min: %.2e, max: %.2e.' % (np.min(y_train[:,i]), np.max(y_train[:,i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Visualisation for when we have 2 input features\n",
    "%matplotlib notebook\n",
    "input_feat_1 = 0\n",
    "input_feat_2 = 1\n",
    "output_feat = 1\n",
    "\n",
    "fig = plt.figure(1, figsize=(8,8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(x_train_norm[:500,input_feat_1], x_train_norm[:500,input_feat_2], \n",
    "           y_train_norm[:500,output_feat])\n",
    "ax.set_xlabel('%s log($M_{H}/M_{S}$)' % (input_features[input_feat_1]))\n",
    "ax.set_ylabel('%s log($M_{H}/M_{S}$)' % (input_features[input_feat_2]))\n",
    "ax.set_zlabel('%s log($M_{G}/M_{S}$)' % (output_features[output_feat]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import binned_statistic\n",
    "a = np.random.normal(size=(100), loc=10.5)\n",
    "# print(a)\n",
    "# print(training_data_dict['real_clustering_data']['stellar_mass_bin_edges'])\n",
    "bin_means, bin_edges, bin_numbers = binned_statistic(a, a, bins=training_data_dict['real_clustering_data']['stellar_mass_bin_edges'], statistic='mean')\n",
    "print(bin_numbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load an existing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = '6x6_all-points_redshifts00-01-02-05-10-20-30-40-60-80_train-test-val080-010-010_tanh_Halo_mass_peak-Scale_peak_mass-Halo_growth_rate-Halo_radius-Redshift_to_Stellar_mass-SFR_test_score4.85e-07'\n",
    "model = load_model(bp_network_dir + model_name + '/model.h5')\n",
    "training_data_dict = pickle.load(open(bp_network_dir + model_name + '/training_data_dict.p', 'rb'))\n",
    "# data_keys = pickle.load(open(bp_network_dir + model_name + '/data_keys.p', 'rb'))\n",
    "\n",
    "position = pickle.load(open(bp_network_dir + model_name + '/best_position.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = '6x6_all-points_redshifts00-01-02-05-10-20-30-40-60-80_train-test-val080-010-010_tanh_Halo_mass_peak-Scale_peak_mass-Halo_growth_rate-Halo_radius-Redshift_to_Stellar_mass-SFR_test_score4.37e-07__fq-ssfr-smf-csfrd-wp_1-1-1-1-1_inertiaStart5_200Explore_new'\n",
    "iteration = '1-42'\n",
    "\n",
    "model = load_model(bp_pso_network_dir + 'real_observations/' + model_name + '/training_best/iteration_{}.h5'.format(iteration))\n",
    "training_data_dict = pickle.load(open(bp_pso_network_dir + 'real_observations/' + model_name + '/full_training_data_dict.p', 'rb'))\n",
    "training_data_dict = make_all_data_train(training_data_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the standard plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = 'train' # 'train', 'val, 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_scores = model.evaluate(x=training_data_dict['input_{}_dict'.format(mode)], y=training_data_dict['output_{}_dict'.format(mode)],\n",
    "                                               sample_weight=training_data_dict['{}_weights'.format(mode)], verbose=1)\n",
    "tot_score = norm_scores[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_points = predict_points(model, training_data_dict, data_type = mode, original_units=True)\n",
    "# title = 'Inputs: {}\\n{:.1e} train points, {} mse {:.3e}, {} data'.format(', '.join(input_features), np.shape(training_data_dict['train_coordinates'])[0], mode, tot_score, mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "##### Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig1 = get_pred_vs_real_scatterplot(model, training_data_dict, 'SFR', data_type=mode,\n",
    "                                   predicted_points = predicted_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Need to carry the halo masses in order to be able to plot this one. Redo the training of a 6x6 network\n",
    "fig2 = get_real_vs_pred_boxplot(model, training_data_dict, predicted_feat = 'Stellar_mass', \n",
    "                                binning_feat = 'Halo_mass', data_type=mode,\n",
    "                                predicted_points = predicted_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig3 = get_halo_stellar_mass_plots(model, training_data_dict, y_max = None, y_min = None,\n",
    "                                    x_min = None, x_max = None, data_type=mode, predicted_points = predicted_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig4 = get_stellar_mass_sfr_plots(model, training_data_dict, title=None, y_max = None, y_min = None,\n",
    "                                    x_min = None, x_max = None, data_type=mode, predicted_points = predicted_points, n_points=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig5 = get_real_vs_pred_boxplot(model, training_data_dict, predicted_feat = 'SFR', \n",
    "                                binning_feat = 'Stellar_mass', data_type=mode,\n",
    "                                predicted_points = predicted_points)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# fig6 = get_real_vs_pred_same_fig(model, training_data_dict, x_axis_feature='Halo_mass', \n",
    "#                                  y_axis_feature = 'Stellar_mass', title=title, data_type=mode, marker_size=5, predicted_points=predicted_points,\n",
    "#                                  y_min=None, y_max=None, x_min=None, x_max=None)\n",
    "fig7 = get_real_vs_pred_same_fig(model, training_data_dict, x_axis_feature='Stellar_mass', \n",
    "                                 y_axis_feature = 'SFR', title=title, data_type=mode, marker_size=2, predicted_points=predicted_points,\n",
    "                                 y_min=None, y_max=None, x_min=None, x_max=None)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig8, fig9 = get_sfr_stellar_mass_contour(model, training_data_dict, unit_dict, title=None, data_type='test',\n",
    "                                 y_min=None, y_max=None, x_min=None, x_max=None, predicted_points=predicted_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for redshift in training_data_dict['unique_redshifts']:\n",
    "    fig88 = get_real_obs_plot(model, training_data_dict, redshift=redshift, title=title, data_type=mode, full_range=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "stellar_mass_bin_width = 0.2\n",
    "fignew = ssfr_emerge_plot(model, training_data_dict, data_type='train', loss_dict={'stellar_mass_bins': np.arange(7, 12.5, stellar_mass_bin_width)})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1.savefig(fig_dir + model_name  +'_true_pred_sfr_scatter.png', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig2.savefig(fig_dir + model_name + '_boxplot_stellar_mass.png', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig3.savefig(fig_dir + model_name +'_scatter_comp_halo_vs_stellar_mass.png', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig4.savefig(fig_dir +'_boxplot_sfr.png', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig5.savefig(fig_dir + model_name +'_scatter_comp_stellar_mass_vs_sfr.png', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "redshift_string = '-'.join(['{:02.0f}'.format(red*10) for red in redshifts])\n",
    "if tot_nr_points == 'all':\n",
    "    nr_points_string = 'all-points'\n",
    "else:\n",
    "    nr_points_string = '{:.1e}points'.format(tot_nr_points)\n",
    "network_name = '{:d}x{:d}_{}_redshifts{}_train-test-val{:03.0f}-{:03.0f}-{:03.0f}_{}_{}_to_{}_{}_score{:.2e}'.format(\n",
    "    network_args['nr_hidden_layers'], network_args['nr_neurons_per_lay'], nr_points_string, redshift_string, train_frac*100, val_frac*100, test_frac*100, network_args['activation_function'], \n",
    "    '-'.join(network_args['input_features']), '-'.join(network_args['output_features']), mode, tot_score\n",
    ")\n",
    "print(network_name)\n",
    "\n",
    "os.makedirs(os.path.dirname(bp_network_dir + network_name + '/model.h5'), exist_ok=True)\n",
    "\n",
    "model.save(bp_network_dir + network_name + '/model.h5')\n",
    "pickle.dump(training_data_dict, open(bp_network_dir + network_name + '/training_data_dict.p', 'wb'))\n",
    "# save the position in weight space for the pso algorithm to use as starting point\n",
    "model_weights = model.get_weights()\n",
    "position = []\n",
    "for weight_matrix in model_weights:\n",
    "    position.extend(np.ndarray.flatten(weight_matrix))\n",
    "position = np.array(position)\n",
    "\n",
    "pickle.dump(position, open(bp_network_dir + network_name + '/best_position.p', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### General parameters\n",
    "nr_repetitions = 1\n",
    "tot_nr_points = 'all' # how many examples will be used for training+validation+testing, 'all' or a number\n",
    "train_frac = 0.8\n",
    "val_frac = 0.1\n",
    "test_frac = 0.1\n",
    "batch_size = 4e4\n",
    "norm = {'input': 'zero_mean_unit_std',\n",
    "        'output': 'none'} # 'none',   'zero_mean_unit_std',   'zero_to_one'\n",
    "input_features = ['Halo_mass_peak', 'Scale_peak_mass', 'Halo_growth_rate', 'Halo_radius', 'Redshift']\n",
    "output_features = ['Stellar_mass', 'SFR']\n",
    "redshifts = [0,.1,.2,.5,1,2,3,4,6,8]\n",
    "same_n_points_per_redshift = False\n",
    "\n",
    "outputs_to_weigh = ['Stellar_mass']\n",
    "weigh_by_redshift = True\n",
    "\n",
    "nr_epochs = 5e4\n",
    "\n",
    "early_stop_patience = 50\n",
    "early_stop_monitor = 'val_loss'\n",
    "early_stop_min_delta = 1e-16\n",
    "\n",
    "validation_data = 'val' #'val' is normally used, use 'train' to check overfitting potential\n",
    "\n",
    "### Network parameters\n",
    "network_args = {        \n",
    "    'nr_hidden_layers': 5,\n",
    "    'nr_neurons_per_lay': 5,\n",
    "    'input_features': input_features,\n",
    "    'output_features': output_features,\n",
    "    'activation_function': 'tanh', # 'tanh', 'leaky_relu'\n",
    "    'output_activation': {'SFR': None, 'Stellar_mass': None},\n",
    "    'reg_strength': 1e-20\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the selected galaxyfile\n",
    "galaxies, data_keys = load_galfiles(redshifts=redshifts, equal_numbers=same_n_points_per_redshift)\n",
    "# prepare the training data\n",
    "training_data_dict = divide_train_data(galaxies, data_keys, network_args, redshifts, outputs_to_weigh=outputs_to_weigh, \n",
    "                                       weigh_by_redshift=weigh_by_redshift, total_set_size=tot_nr_points, train_frac=train_frac, val_frac=val_frac, \n",
    "                                       test_frac=test_frac, emerge_targets=True)\n",
    "# galaxies = None\n",
    "training_data_dict = normalise_data(training_data_dict, norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = standard_network(network_args['input_features'], network_args['output_features'], network_args['nr_neurons_per_lay'], \n",
    "                         network_args['nr_hidden_layers'], network_args['activation_function'], \n",
    "                         network_args['output_activation'], network_args['reg_strength'], clipvalue=.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "earlystop = EarlyStopping(monitor=early_stop_monitor, min_delta=early_stop_min_delta, patience=early_stop_patience, \\\n",
    "                          verbose=1, mode='auto')\n",
    "callbacks_list = [earlystop]\n",
    "\n",
    "\n",
    "for i_rep in range(1, nr_repetitions+1):\n",
    "    \n",
    "    history = model.fit(x = training_data_dict['input_train_dict'], y = training_data_dict['output_train_dict'], \n",
    "                        validation_data = (training_data_dict['input_'+validation_data+'_dict'], \n",
    "                        training_data_dict['output_'+validation_data+'_dict'], training_data_dict['val_weights']), \n",
    "                        epochs=int(nr_epochs), batch_size=int(batch_size), callbacks=callbacks_list,\n",
    "                        sample_weight=training_data_dict['train_weights'], verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot loss history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all data in history\n",
    "print(history.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for loss\n",
    "%matplotlib inline\n",
    "fig = plt.figure(5, figsize=(8,8))\n",
    "plt.plot(history.history['loss'], 'b')\n",
    "plt.plot(history.history['val_loss'], 'r')\n",
    "plt.yscale('log')\n",
    "#plt.title(title)\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input parameter search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do a batch run to see which input parameters gives the best score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### General parameters\n",
    "nr_steps = 5e4\n",
    "batch_size = .5e4\n",
    "total_set_size = 3e4 # how many examples will be used for training+validation+testing\n",
    "train_size = 2.5e4\n",
    "val_size = .25e4\n",
    "test_size = .25e4\n",
    "norm = {'input': 'zero_mean_unit_std',\n",
    "        'output': 'none'} # 'none',   'zero_mean_unit_std',   'zero_to_one'\n",
    "output_features = ['Stellar_mass', 'SFR']\n",
    "redshifts = [0,.1,.2,.5,1,2,3,4,6,8]\n",
    "same_n_points_per_redshift = True\n",
    "\n",
    "outputs_to_weigh = ['Stellar_mass']\n",
    "weigh_by_redshift = True\n",
    "\n",
    "nr_epochs = nr_steps * batch_size / train_size\n",
    "\n",
    "early_stop_patience = 20\n",
    "early_stop_monitor = 'val_loss'\n",
    "early_stop_min_delta = 1e-16\n",
    "\n",
    "### Network parameters\n",
    "nr_layers = 10\n",
    "activation_function = 'tanh'\n",
    "output_activation = {'SFR': None, 'Stellar_mass': None}\n",
    "neurons_per_layer = 10\n",
    "reg_strength = 0#1e-5\n",
    "\n",
    "# Batch run parameters\n",
    "core_input_features = ['Halo_mass', 'Redshift']\n",
    "tested_input_features = ['Scale_half_mass', 'Halo_growth_rate', 'Halo_mass_peak', 'Scale_peak_mass']\n",
    "output_features = ['Stellar_mass', 'SFR']\n",
    "nr_extra_params_list = [1, 2, 3, 4]\n",
    "nr_runs_per_comb = 3\n",
    "\n",
    "\n",
    "verb = 0 # prints progress to stdout\n",
    "\n",
    "nr_epochs = nr_steps * batch_size / train_size\n",
    "parameter_dictionary = {\n",
    "    'fixed_input_features': core_input_features,\n",
    "    'tested_input_features': tested_input_features,\n",
    "    'output_features': output_features,\n",
    "    'nr_extra_parameter_combinations': nr_extra_params_list,\n",
    "    'nr_steps': [nr_steps],\n",
    "    'batch_size': [batch_size],\n",
    "    'nr_epochs': [nr_epochs],\n",
    "    'nr_training_samples': [train_size],\n",
    "    'nr_validation_samples': [val_size],\n",
    "    'nr_test_samples': [test_size],\n",
    "    'data_normalization': norm,\n",
    "    'activation_function': activation_function,\n",
    "    'neurons_per_layer': [neurons_per_layer],\n",
    "    'nr_hidden_layers': [nr_layers],\n",
    "    'output_activation_function': output_activation,\n",
    "    'description': 'Each parameter setting is represented by one list containing three objects. The first one is ' + \\\n",
    "    'the input parameters. The second one is the mse test scores obtained for the different runs (sum of test losses).'+\\\n",
    "    ' The third one is the ' +\\\n",
    "    'loss histories for the different runs [training_loss, validation_loss].'\n",
    "}\n",
    "results_list = [parameter_dictionary]\n",
    "nr_combs_total = 0\n",
    "for nr_extra_params in nr_extra_params_list:\n",
    "    nr_combs_total += comb(len(tested_input_features), nr_extra_params)\n",
    "comb_counter = 0 # to keep track of how many combinations I've gone through\n",
    "\n",
    "with open('model_comparisons/progress.txt', 'w+') as f:\n",
    "    \n",
    "    date_string_proper = datetime.datetime.now().strftime(\"%H:%M, %Y-%m-%d\")\n",
    "    f.write('Benchmark done on input parameters at ' + date_string_proper + '\\n\\n')\n",
    "    f.flush()\n",
    "    \n",
    "    # load the selected galaxyfile\n",
    "    galaxies, data_keys, unit_dict = load_galfiles(redshifts=redshifts, equal_numbers=same_n_points_per_redshift)\n",
    "    \n",
    "    for i_nr_extra_params, nr_extra_params in enumerate(nr_extra_params_list):\n",
    "        \n",
    "        extra_param_combs = list(combinations(tested_input_features, nr_extra_params))\n",
    "        \n",
    "        date_string_proper = datetime.datetime.now().strftime(\"%H:%M, %Y-%m-%d\")\n",
    "        f.write(date_string_proper + '    Testing %d extra parameters. %d/%d extra parameter count tested. \\n\\n' %\n",
    "                (nr_extra_params, i_nr_extra_params+1, len(nr_extra_params_list)))\n",
    "        f.flush()\n",
    "    \n",
    "        for i_comb, param_comb in enumerate(extra_param_combs):\n",
    "            input_features = core_input_features.copy()\n",
    "            input_features.extend(param_comb)\n",
    "            \n",
    "\n",
    "            # prepare the training data\n",
    "            training_data_dict = divide_train_data(galaxies, data_keys, input_features, output_features, redshifts,\n",
    "                                                   int(total_set_size), int(train_size), int(val_size), int(test_size))\n",
    "            training_data_dict = normalise_data(training_data_dict, norm)\n",
    "            \n",
    "            earlystop = EarlyStopping(monitor=early_stop_monitor, min_delta=early_stop_min_delta, \n",
    "                                      patience=early_stop_patience, verbose=1, mode='auto')\n",
    "            callbacks_list = [earlystop]\n",
    "\n",
    "            train_weights, val_weights, test_weights = get_weights(training_data_dict, output_features, outputs_to_weigh, \n",
    "                                                     weigh_by_redshift=weigh_by_redshift)\n",
    "            \n",
    "            date_string_proper = datetime.datetime.now().strftime(\"%H:%M, %Y-%m-%d\")\n",
    "            comb_counter += 1\n",
    "            f.write(date_string_proper + '        Testing combination %d/%d. \\n\\n' % (comb_counter, nr_combs_total))\n",
    "            f.flush()\n",
    "            \n",
    "            scores = []\n",
    "            histories = []\n",
    "\n",
    "            for i_run in range(nr_runs_per_comb):\n",
    "\n",
    "                # create model\n",
    "                model = standard_network(input_features, output_features, neurons_per_layer, nr_layers, \n",
    "                                         activation_function, output_activation, reg_strength)\n",
    "\n",
    "                # Fit the model\n",
    "\n",
    "                history = model.fit(x = training_data_dict['input_train_dict'], y = training_data_dict['output_train_dict'], \n",
    "                                    validation_data = (training_data_dict['input_val_dict'], \n",
    "                                    training_data_dict['output_val_dict'], val_weights), \n",
    "                                    epochs=int(nr_epochs), batch_size=int(batch_size), callbacks=callbacks_list,\n",
    "                                    sample_weight=train_weights, verbose=verb)\n",
    "                score = model.evaluate(x=training_data_dict['input_test_dict'], y=training_data_dict['output_test_dict'], \n",
    "                                       sample_weight=test_weights, verbose=verb)\n",
    "                scores.append(score[0]) # take only the sum of the output losses (total loss)\n",
    "                    \n",
    "\n",
    "                    \n",
    "                histories.append([history.history['loss'], history.history['val_loss']])\n",
    "                \n",
    "            results_list.append([input_features, scores, histories])\n",
    "            \n",
    "    date_string_proper = datetime.datetime.now().strftime(\"%H:%M, %Y-%m-%d\")\n",
    "    f.write('Benchmark completed at ' + date_string_proper + '\\n')\n",
    "    f.flush()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save the result\n",
    "date_string = datetime.datetime.now().strftime('%Y-%m-%d--%H-%M-%S')\n",
    "custom_string = '3-6_total_inputs_redshift_growth'\n",
    "tot_string = date_string + '-' + custom_string\n",
    "\n",
    "with open('model_comparisons/' + tot_string + '.json', 'w+') as f:\n",
    "    json.dump(results_list, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#print(train_loss)\n",
    "print('Lowest train/val/test loss: %.2f, %.2f, %.2f' % (np.amin(train_loss), np.amin(val_loss), np.amin(test_loss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load a batch run result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load a result\n",
    "loaded_list_string = '2018-06-20--09-52-20-3-6_total_inputs_redshift'\n",
    "with open('model_comparisons/' + loaded_list_string + '.json', 'r') as f:\n",
    "    results_list = json.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(results_list[1][0]))\n",
    "print(results_list[1][0])\n",
    "results_list[0]['nr_extra_parameter_combinations']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Plot the loss histories to make sure that the best performance was reached\n",
    "\n",
    "for comb_nr, lst in enumerate(results_list[1:]):\n",
    "\n",
    "    \n",
    "    train_loss = lst[2][0][0]\n",
    "    val_loss = lst[2][0][1]\n",
    "    test_losses = lst[1]\n",
    "    avg_test_loss = np.mean(test_losses)\n",
    "    title = ', '.join(lst[0]) + '\\naverage test loss: {:.3e}'.format(avg_test_loss)\n",
    "\n",
    "    #print('Lowest train/val/test loss: %.2e, %.2e, %.2e' % (np.amin(train_loss), np.amin(val_loss), np.amin(test_loss)))\n",
    "\n",
    "    # summarize history for loss\n",
    "    fig = plt.figure(5, figsize=(8,8))\n",
    "    for i_run in range(len(test_losses)):\n",
    "        train_loss = lst[2][i_run][0]\n",
    "        val_loss = lst[2][i_run][1]\n",
    "        plt.plot(train_loss, 'b')\n",
    "        plt.plot(val_loss, 'r')\n",
    "    plt.yscale('log')\n",
    "    plt.title(title)\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'val'], loc='upper right')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results_list[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### Get the X best performing input parameters\n",
    "top_nr_of_parameter_combs = 10\n",
    "column_widths = [0.2, 0.1]\n",
    "test_results = []\n",
    "for comb_nr, lst in enumerate(results_list[1:]):\n",
    "    best_test = np.amin(lst[1])\n",
    "    test_results.append(best_test)\n",
    "    \n",
    "best_test_indices = np.argsort(test_results)\n",
    "\n",
    "fig1362 = plt.figure(figsize=(16,8))\n",
    "\n",
    "ax = plt.subplot(1,1,1)\n",
    "\n",
    "nr_fixed_inputs = len(results_list[0]['fixed_input_features'])\n",
    "collabel=('Input parameters', 'best test mse')\n",
    "table_vals = []\n",
    "for i in range(top_nr_of_parameter_combs):\n",
    "    \n",
    "    inputs = ', '.join(results_list[best_test_indices[i]+1][0][:nr_fixed_inputs])\n",
    "    inputs = inputs + '\\n+\\n' + ', '.join(results_list[best_test_indices[i]+1][0][nr_fixed_inputs:])\n",
    "    \n",
    "    table_vals.append([inputs, '{:.2e} (mean: {:.2e})'.format(test_results[best_test_indices[i]], \n",
    "                                                              np.mean(results_list[best_test_indices[i]+1][1]))])\n",
    "\n",
    "the_table = ax.table(cellText=table_vals,colLabels=collabel,colWidths=column_widths,loc='center')\n",
    "the_table.set_fontsize(15)\n",
    "the_table.scale(3, 4)\n",
    "ax.axis('tight')\n",
    "ax.axis('off')\n",
    "plt.title('# weight updates: %.1e, batch size: %.1e,\\nnorm: %s, output norm: False, 10 layers, 30 neurons per layer' % \n",
    "          (results_list[0]['nr_steps'][0], results_list[0]['batch_size'][0], results_list[0]['data_normalization']) +\n",
    "           '\\nFixed input parameters: %s' % (', '.join(results_list[0]['fixed_input_features'])), \n",
    "          fontsize=20)\n",
    "ttl = ax.title\n",
    "ttl.set_position([0.5, 1.15])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1362.savefig(fig_dir + '3-6_total_inputs_redshift_growth' + '_param_comb_scores_test.png', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Find out how a parameter affects the result\n",
    "nr_columns = 10\n",
    "collabel = ['Input parameter']\n",
    "[collabel.append('{:d}'.format(i)) for i in range(1, nr_columns+1)]\n",
    "collabel = tuple(collabel)\n",
    "#collabel = ('Input parameter', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10')\n",
    "first_col_width = .1\n",
    "last_col_width = .02\n",
    "column_widths = [first_col_width]\n",
    "for i in range(nr_columns):\n",
    "    column_widths.append(last_col_width)\n",
    "tab_rows = []\n",
    "row_colours = []\n",
    "\n",
    "for param in results_list[0]['tested_input_features']:\n",
    "    \n",
    "    tab_row = [param]\n",
    "    row_colour = ['w']\n",
    "\n",
    "    for i, ind in enumerate(best_test_indices[:nr_columns]):\n",
    "        inputs = results_list[ind+1][0]\n",
    "        if param in inputs:\n",
    "            tab_row.append('Yes')\n",
    "            row_colour.append('g')\n",
    "        else:\n",
    "            tab_row.append('No')\n",
    "            row_colour.append('r')\n",
    "\n",
    "    tab_rows.append(tab_row)\n",
    "    row_colours.append(row_colour)\n",
    "        \n",
    "fig8845 = plt.figure(figsize=(16,8))\n",
    "\n",
    "ax = plt.subplot(111)\n",
    "\n",
    "the_table = ax.table(cellText=tab_rows,colLabels=collabel,colWidths=column_widths,\n",
    "                     cellColours=row_colours,loc='center')\n",
    "the_table.set_fontsize(15)\n",
    "the_table.scale(3, 3)\n",
    "\n",
    "plt.subplots_adjust(left=None, bottom=None, right=None, top=None,\n",
    "                wspace=0.2, hspace=0.5)\n",
    "plt.title('# weight updates: %.1e, batch size: %.1e,\\nnorm: %s, output norm: False, 10 layers, 30 neurons per layer'\n",
    "          % (results_list[0]['nr_steps'][0], results_list[0]['batch_size'][0], results_list[0]['data_normalization'])+\n",
    "           '\\nFixed input parameters: %s\\nOrdering based on best out of 3 test scores' % \n",
    "          (', '.join(results_list[0]['fixed_input_features'])), fontsize=20)\n",
    "#plt.title('Ordering based on validation scores', fontsize=20)\n",
    "ttl = ax.title\n",
    "ttl.set_position([0.5, 0.9])\n",
    "\n",
    "#ax.axis('tight')\n",
    "ax.axis('off')\n",
    "\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig8845.savefig(fig_dir + '3-6_total_inputs_redshift_growth' + '_param_importance_test.png', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter search multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Multiprocessing version\n",
    "nr_steps = 1e5\n",
    "\n",
    "validation_data = 'val' #'val' is normally used, use 'train' to check overfitting potential\n",
    "\n",
    "nr_folds = 3\n",
    "tot_nr_points = 2e5 # how many examples will be used for training+validation+testing, 'all' or a number\n",
    "train_frac = 0.8\n",
    "val_frac = 0.1\n",
    "test_frac = 0.1\n",
    "\n",
    "input_features = ['Halo_mass_peak', 'Scale_peak_mass', 'Halo_growth_rate', 'Halo_radius', 'Redshift']\n",
    "output_features = ['Stellar_mass', 'SFR']\n",
    "outputs_to_weigh = ['Stellar_mass']\n",
    "redshifts = [0,.1,.2,.5,1,2,3,4,6,8]\n",
    "same_n_points_per_redshift = False\n",
    "weigh_by_redshift = True\n",
    "\n",
    "norm = {'input': 'zero_mean_unit_std',\n",
    "        'output': 'none'} # 'none',   'zero_mean_unit_std',   'zero_to_one'\n",
    "\n",
    "nr_layers = [6, 7, 8, 9, 10, 12, 14, 16, 20]\n",
    "neurons_per_layer = [6, 7, 8, 9, 10, 12, 14, 16, 20]\n",
    "activation_functions = ['tanh']\n",
    "\n",
    "loss_function = 'mse'\n",
    "output_activation = {'SFR': None, 'Stellar_mass': None}\n",
    "reg_strengths = np.array([1e-20]) # np.power(10, np.random.uniform(low=-20, high=-10, size=5)) \n",
    "\n",
    "batch_sizes = np.array([4e4]) # np.power(10, np.random.uniform(low=4, high=5, size=5))\n",
    "nr_epochs = nr_steps * batch_sizes / (tot_nr_points*train_frac)\n",
    "early_stop_patience = nr_epochs / 10\n",
    "early_stop_min_delta = 1e-20\n",
    "\n",
    "progress_file = 'hyperparameter_searches/progress.txt'\n",
    "n_processes = 27\n",
    "\n",
    "verb = 0 # prints progress to stdout\n",
    "\n",
    "parameter_dictionary = {\n",
    "    'input_features': input_features,\n",
    "    'output_features': output_features,\n",
    "    'outputs_to_weigh': outputs_to_weigh,\n",
    "    'loss_function': loss_function,\n",
    "    'batch_sizes': batch_sizes.tolist(),\n",
    "    'nr_epochs': nr_epochs.tolist(),\n",
    "    'tot_nr_points': [tot_nr_points],\n",
    "    'reg_strengths': reg_strengths.tolist(),\n",
    "    'nr_folds': nr_folds,\n",
    "    'input_norms': input_norms,\n",
    "    'activation_functions': activation_functions,\n",
    "    'neurons_per_layer': neurons_per_layer,\n",
    "    'nr_hidden_layers': nr_layers,\n",
    "    'output_activation_function': 'none',\n",
    "    'description': 'Each parameter setting is represented by one list containing five objects: [parameters, test_score, train_history, '+\\\n",
    "    'val_history, param_id]. The first one is ' + \\\n",
    "    'the parameters of the model. The second one is the test score of the final model'+\\\n",
    "    ' ' +\\\n",
    "    'The third one is the training' +\\\n",
    "    'loss history and the fourth one is the validation loss history. '+\\\n",
    "    'The fifth one is an id for the parameter combinations. If there are several runs for the same combs then they will have the same id.'\n",
    "}\n",
    "results_list = [parameter_dictionary]\n",
    "tot_nr_runs = len(nr_layers) * len(activation_functions) * len(input_norms) * \\\n",
    "                len(batch_sizes) * len(reg_strengths) * nr_folds\n",
    "\n",
    "\n",
    "    \n",
    "with open(progress_file, 'w') as f:\n",
    "\n",
    "    date_string_proper = datetime.datetime.now().strftime(\"%H:%M, %Y-%m-%d\")\n",
    "    f.write('Benchmark done on hyperparameters on ' + date_string_proper + '\\n\\n' + '0/{}'.format(tot_nr_runs))\n",
    "    f.flush()\n",
    "\n",
    "# load the selected galaxyfile, be sure that the autoloaded galaxyfile is the one that you want to use!!\n",
    "galaxies, data_keys = load_galfiles(redshifts=redshifts, equal_numbers=same_n_points_per_redshift)\n",
    "\n",
    "param_combos = []\n",
    "param_id = 0      # for identifying the same combinations of parameters\n",
    "\n",
    "for i_act_fun, act_fun in enumerate(activation_functions):\n",
    "    for i_reg_strength, reg_strength in enumerate(reg_strengths):\n",
    "        for i_neur_per_lay, neur_per_lay in enumerate(neurons_per_layer):\n",
    "#                 for i_nr_lay, nr_lay in enumerate(nr_layers):\n",
    "            for i_batch_size, batch_size in enumerate(batch_sizes):\n",
    "\n",
    "                param_id += 1\n",
    "                for fold in range(nr_folds):\n",
    "\n",
    "                    \n",
    "                    ### Network parameters\n",
    "                    network_args = {\n",
    "                        'nr_hidden_layers': nr_layers[i_neur_per_lay],\n",
    "                        'nr_neurons_per_lay': neur_per_lay,\n",
    "                        'input_features': input_features,\n",
    "                        'output_features': output_features,\n",
    "                        'activation_function': act_fun,\n",
    "                        'output_activation': output_activation,\n",
    "                        'reg_strength': reg_strength\n",
    "                    }\n",
    "\n",
    "                    params = [galaxies, network_args, data_keys, redshifts, tot_nr_points, train_frac, val_frac, test_frac, outputs_to_weigh, weigh_by_redshift, \n",
    "                              norm, nr_epochs[i_batch_size], \n",
    "                              batch_size, progress_file, verb, early_stop_min_delta, \n",
    "                              early_stop_patience[i_batch_size], param_id]\n",
    "\n",
    "                    param_combos.append(params)\n",
    "\n",
    "write_lock = mp.Lock()\n",
    "with mp.Pool(processes=n_processes, initializer=init, initargs=(write_lock,)) as pool:\n",
    "    results = pool.map(train_net, param_combos)\n",
    "\n",
    "results_list.extend(results)\n",
    "\n",
    "with open(progress_file, 'a') as f:\n",
    "    date_string_proper = datetime.datetime.now().strftime(\"%H:%M, %Y-%m-%d\")\n",
    "    f.write('\\nBenchmark completed at ' + date_string_proper + '\\n')\n",
    "    f.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save the result\n",
    "date_string = datetime.datetime.now().strftime('%Y-%m-%d--%H-%M-%S')\n",
    "custom_string = 'network_size'\n",
    "tot_string = date_string + '_' + custom_string + '.json'\n",
    "with open('hyperparameter_searches/' + tot_string, 'w+') as f:\n",
    "    json.dump(results_list, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load a result\n",
    "loaded_list_string = '2018-09-21--11-21-31_broad_sweep'\n",
    "with open('hyperparameter_searches/' + loaded_list_string + '.json', 'r') as f:\n",
    "    results_list = json.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(results_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Plot the loss histories to make sure that the best performance was reached\n",
    "contains_data_list = [True if lst[1] else False for lst in results_list[1:]]\n",
    "print('Number of parameter combinations that ended up exploding before test loss could be evaluated: {:d}'.format(\n",
    "      np.sum(not contains_data_list)))\n",
    "print('Number of parameter combinations that contains at least one test loss data point: {:d}'.format(\n",
    "      np.sum(contains_data_list)))\n",
    "for comb_nr, lst in enumerate(results_list[1:]):\n",
    "    \n",
    "    if lst[1]:\n",
    "        title = str(lst[0]) + '\\n final test score: {:.2e}'.format(lst[1])\n",
    "        train_losses = lst[2]\n",
    "        val_losses = lst[3]\n",
    "\n",
    "        #print('Lowest train/val/test loss: %.2e, %.2e, %.2e' % (np.amin(train_loss), np.amin(val_loss), np.amin(test_loss)))\n",
    "\n",
    "        # summarize history for loss\n",
    "        fig = plt.figure(5, figsize=(12,12))\n",
    "        plt.plot(range(1, len(train_losses)+1), train_losses, 'b')\n",
    "        plt.plot(range(1, len(val_losses)+1), val_losses, 'r')\n",
    "        plt.yscale('log')\n",
    "        plt.title(title)\n",
    "        plt.ylabel('loss')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'val'], loc='upper right')\n",
    "        plt.show()\n",
    "  #  else:\n",
    "  #      print('No data here, boss')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{'inp_norm': inp_norm, 'nr_lay': nr_lay, 'neur_per_lay': neur_per_lay, \n",
    "                                         'act_fun': act_fun, 'batch_size': batch_size, 'reg_strength': reg_strength}\n",
    "nr_lay_nans = []\n",
    "neur_per_lay_nans = []\n",
    "batch_size_nans = []\n",
    "reg_strength_nans = []\n",
    "inp_norm_nans = []\n",
    "actfun_nans = []\n",
    "\n",
    "for i_lst, lst in enumerate(results_list[1:]):\n",
    "\n",
    "   # print(lst[2][0])\n",
    "  #  print(len(lst[2][0]))\n",
    "    if len(lst[2][0])%10 > 0:\n",
    "        \n",
    "        nr_lay_nans.append(lst[0]['nr_lay'])\n",
    "        neur_per_lay_nans.append(lst[0]['neur_per_lay'])\n",
    "        batch_size_nans.append(lst[0]['batch_size'])\n",
    "        reg_strength_nans.append(lst[0]['reg_strength'])\n",
    "        inp_norm_nans.append(lst[0]['inp_norm'])\n",
    "        actfun_nans.append(lst[0]['act_fun'])\n",
    "        \n",
    "    #    print('object nr {}'.format(i_lst+1))\n",
    "    \n",
    "print('The occurrences of NaN value in the losses by input parameter')\n",
    "print('nr_lays: ', dict(Counter(nr_lay_nans)))\n",
    "print('neur_per_lay: ', dict(Counter(neur_per_lay_nans)))\n",
    "print('batch_size: ', dict(Counter(batch_size_nans)))\n",
    "print('reg_strength: ', dict(Counter(reg_strength_nans)))\n",
    "print('inp_norm: ', dict(Counter(inp_norm_nans)))\n",
    "print('act_fun: ', dict(Counter(actfun_nans)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "['results_list[key] results_list[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "top_nr_of_parameter_combs = 50\n",
    "column_widths = [.05] * 7\n",
    "test_results = []\n",
    "for comb_nr, lst in enumerate(results_list[1:]):\n",
    "    test_result = lst[1]\n",
    "    test_results.append(test_result)\n",
    "sort_indices = np.argsort(test_results)\n",
    "\n",
    "fig = plt.figure(figsize=(16,8))\n",
    "ax = plt.subplot(1,1,1)\n",
    "\n",
    "collabel=('nr', 'nr_lay', 'nr_neur', 'act_fun', 'batch_size', 'reg_strength', 'test mse')\n",
    "table_vals = []\n",
    "for i in range(top_nr_of_parameter_combs):\n",
    "    \n",
    "#     'nr_lay': network_args['nr_hidden_layers'], \n",
    "#                   'neur_per_lay': network_args['nr_neurons_per_lay'], \n",
    "#                   'act_fun': network_args['activation_function'], 'batch_size': batch_size, \n",
    "#                   'reg_strength': network_args['reg_strength']\n",
    "    \n",
    "#     parameters = ', '.join(results_list[sort_indices[i]+1][0][:nr_fixed_inputs])\n",
    "#     inputs = inputs + '\\n+\\n' + ', '.join(results_list[best_test_indices[i]+1][0][nr_fixed_inputs:])\n",
    "    \n",
    "    table_vals.append([i+1, results_list[sort_indices[i]+1][0]['nr_lay'], results_list[sort_indices[i]+1][0]['neur_per_lay'], results_list[sort_indices[i]+1][0]['act_fun'], \n",
    "                       '{:.2e}'.format(results_list[sort_indices[i]+1][0]['batch_size']), '{:.2e}'.format(results_list[sort_indices[i]+1][0]['reg_strength']), \n",
    "                       '{:.2e}'.format(results_list[sort_indices[i]+1][1])])\n",
    "\n",
    "the_table = ax.table(cellText=table_vals,colLabels=collabel,colWidths=column_widths,loc='center')\n",
    "the_table.set_fontsize(15)\n",
    "the_table.scale(3, 4)\n",
    "ax.axis('tight')\n",
    "ax.axis('off')\n",
    "plt.title('nr_layers: ' + ', '.join([str(temp) for temp in results_list[0]['nr_hidden_layers']])\n",
    "          + '\\nnr_neur_per_lay: ' + ', '.join([str(temp) for temp in results_list[0]['neurons_per_layer']])\n",
    "          + '\\nact_funs: ' + ', '.join(results_list[0]['activation_functions'])\n",
    "          + '\\nbatch_sizes: ' + ', '.join(['{:.2e}'.format(temp) for temp in sorted(results_list[0]['batch_sizes'])])\n",
    "          + '\\nreg_strengts: ' + ', '.join(['{:.2e}'.format(temp) for temp in sorted(results_list[0]['reg_strengths'])]),\n",
    "          fontsize=20)\n",
    "ttl = ax.title\n",
    "ttl.set_position([0.5, 3.4])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig('hyperparameter_searches/' + loaded_list_string + '_top{:d}.png'.format(top_nr_of_parameter_combs), bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# On preprocessed data\n",
    "test_loss, test_mse = model.evaluate(x_test_norm, y_test_norm, verbose=0)\n",
    "print('MSE for the processed data: %.4f' % (test_mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Predict real value of points\n",
    "if norm == 'zero_mean_unit_std':\n",
    "    predicted_norm_points = model.predict(training_data_dict['x_test_norm'])\n",
    "    predicted_points = predicted_norm_points * training_data_dict['y_data_stds'] + training_data_dict['y_data_means']\n",
    "    \n",
    "if norm == 'zero_to_one':\n",
    "    predicted_norm_points = model.predict(training_data_dict['x_test_norm'])\n",
    "    predicted_points = predicted_norm_points * (training_data_dict['y_data_max'] - training_data_dict['y_data_min']) + \\\n",
    "                        training_data_dict['y_data_min']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get mse for the real predictions\n",
    "n_points = np.shape(predicted_points)[0]\n",
    "x_minus_y = predicted_points - y_test\n",
    "\n",
    "feature_scores = np.sum(np.power(x_minus_y, 2), 0) / n_points\n",
    "total_score = np.sum(feature_scores) / 2\n",
    "\n",
    "print('MSE for the unprocessed data: %.4f' % (total_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save the model if it is useful\n",
    "importlib.reload(model_management)\n",
    "model_dictionary = {\n",
    "    'training_method': 'backprop',\n",
    "    'input_features': input_features,\n",
    "    'output_features': output_features,\n",
    "    'number_of_epochs': nEpochs,\n",
    "    'batch_size': batchSize,\n",
    "    'number_of_layers': nLayers,\n",
    "    'neurons_per_layer': neuronsPerLayer,\n",
    "    'activation_function': activationFunction,\n",
    "    'train_set_size': train_size,\n",
    "    'loss_function': loss_function,\n",
    "    'test_loss': test_loss,\n",
    "    'test_mse': test_mse,\n",
    "    'preprocess_data': preprocess_data\n",
    "}\n",
    "description = 'First network trained on preprocessed data.'\n",
    "model_management.SaveModel(model, model_dictionary, description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "#x1 = np.linspace(np.min(x_test[:,0]), np.max(x_test[:,0]), 30)\n",
    "#x2 = np.linspace(np.min(x_test[:,1]), np.max(x_test[:,1]), 30)\n",
    "#X1, X2 = np.meshgrid(x1, x2)\n",
    "#Z = np.zeros(X1.shape)\n",
    "#for i in range(30):\n",
    "#    for j in range(30):\n",
    "#        Z[i, j] = model.predict(np.array([X1[i,j], X2[i,j]])) TODO varfr funkar inte det hr??\n",
    "        \n",
    "#fig = plt.figure(4)\n",
    "#ax = plt.axes(projection='3d')\n",
    "#ax.contour3D(X, Y, Z, 50, cmap='binary')\n",
    "#ax.set_xlabel('x')\n",
    "#ax.set_ylabel('y')\n",
    "#ax.set_zlabel('z')\n",
    "        \n",
    "### Old visualisation way\n",
    "### Visualisation of prediction strength for when we have 2 input features\n",
    "if plot_threeD and len(input_features) == 2:\n",
    "    predictedY = model.predict(x_test_norm)\n",
    "    predictedY = predictedY * y_data_stds + y_data_means\n",
    "    fig = plt.figure(2, figsize=(8,8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.scatter(x_test[:,0], x_test[:,1], \n",
    "               y_test[:,0], s=3)\n",
    "    ax.scatter(x_test[:,0], x_test[:,1], \n",
    "               predictedY, s=3)\n",
    "    ax.set_xlabel('%s log($M_{H}/M_{S}$)' % (input_features[0]))\n",
    "    ax.set_ylabel('%s log($M_{H}/M_{S}$)' % (input_features[1]))\n",
    "    ax.set_zlabel('%s log($M_{G}/M_{S}$)' % (output_features[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check to see how the MSE is calculated "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_points = model.predict(x_test)\n",
    "print(np.shape(predicted_points))\n",
    "n_points = np.shape(predicted_points)[0]\n",
    "x_minus_y = predicted_points - y_test\n",
    "\n",
    "feature_scores = np.sum(np.power(x_minus_y, 2), 0) / n_points\n",
    "total_score = np.sum(feature_scores) / 2\n",
    "\n",
    "print(total_score)\n",
    "\n",
    "keras_scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(keras_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for loss\n",
    "%matplotlib inline\n",
    "fig = plt.figure(5, figsize=(8,8))\n",
    "plt.plot(history.history['loss'], 'b')\n",
    "plt.plot(history.history['val_loss'], 'r')\n",
    "plt.yscale('log')\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter search, GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### single process, GPU version\n",
    "keep_writing_to_old_file = True\n",
    "file_name = '2018-09-24_network_size' # file to keep working on if keep_writing_to_old_file=True\n",
    "\n",
    "nr_steps = 1e5 # 1e5 in full run\n",
    "\n",
    "validation_data = 'val' #'val' is normally used, use 'train' to check overfitting potential\n",
    "\n",
    "nr_folds = 3\n",
    "tot_nr_points = 'all' # how many examples will be used for training+validation+testing, 'all' or a number\n",
    "train_frac = 0.8\n",
    "val_frac = 0.1\n",
    "test_frac = 0.1\n",
    "\n",
    "input_features = ['Halo_mass_peak', 'Scale_peak_mass', 'Halo_growth_rate', 'Halo_radius', 'Redshift']\n",
    "output_features = ['Stellar_mass', 'SFR']\n",
    "outputs_to_weigh = ['Stellar_mass']\n",
    "redshifts = [0,.1,.2,.5,1,2,3,4,6,8]\n",
    "same_n_points_per_redshift = False\n",
    "weigh_by_redshift = True\n",
    "\n",
    "norm = {'input': 'zero_mean_unit_std',\n",
    "        'output': 'none'} # 'none',   'zero_mean_unit_std',   'zero_to_one'\n",
    "\n",
    "nr_layers = [3, 4, 5, 6, 7, 8, 9, 10, 12, 14, 16, 20]\n",
    "neurons_per_layer = [3, 4, 5, 6, 7, 8, 9, 10, 12, 14, 16, 20]\n",
    "activation_functions = ['tanh']\n",
    "\n",
    "loss_function = 'mse'\n",
    "output_activation = {'SFR': None, 'Stellar_mass': None}\n",
    "reg_strengths = np.array([1e-20]) # np.power(10, np.random.uniform(low=-20, high=-10, size=5)) \n",
    "\n",
    "batch_sizes = np.array([4e4]) # np.power(10, np.random.uniform(low=4, high=5, size=5))\n",
    "if isinstance(tot_nr_points, float):\n",
    "    nr_epochs = nr_steps * batch_sizes / (tot_nr_points*train_frac)\n",
    "else:\n",
    "    nr_epochs = nr_steps * batch_sizes / (1.29e6*train_frac)\n",
    "early_stop_patience = [30]\n",
    "early_stop_min_delta = 1e-20\n",
    "\n",
    "progress_file = 'hyperparameter_searches/progress.txt'\n",
    "\n",
    "verb = 0 # prints progress to stdout\n",
    "\n",
    "parameter_dictionary = {\n",
    "    'input_features': input_features,\n",
    "    'output_features': output_features,\n",
    "    'outputs_to_weigh': outputs_to_weigh,\n",
    "    'loss_function': loss_function,\n",
    "    'batch_sizes': batch_sizes.tolist(),\n",
    "    'nr_epochs': nr_epochs.tolist(),\n",
    "    'tot_nr_points': [tot_nr_points],\n",
    "    'reg_strengths': reg_strengths.tolist(),\n",
    "    'nr_folds': nr_folds,\n",
    "    'activation_functions': activation_functions,\n",
    "    'neurons_per_layer': neurons_per_layer,\n",
    "    'nr_hidden_layers': nr_layers,\n",
    "    'output_activation_function': 'none',\n",
    "    'description': 'Each parameter setting is represented by one list containing five objects: [parameters, test_score, train_history, '+\\\n",
    "    'val_history, param_id]. The first one is ' + \\\n",
    "    'the parameters of the model. The second one is the test score of the final model'+\\\n",
    "    ' ' +\\\n",
    "    'The third one is the training' +\\\n",
    "    'loss history and the fourth one is the validation loss history. '+\\\n",
    "    'The fifth one is an id for the parameter combinations. If there are several runs for the same combs then they will have the same id.'\n",
    "}\n",
    "tot_nr_runs = len(nr_layers) * len(activation_functions) * \\\n",
    "                len(batch_sizes) * len(reg_strengths) * nr_folds\n",
    "\n",
    "run_counter = 0 # to keep track of how many combinations I've gone through\n",
    "\n",
    "if keep_writing_to_old_file:\n",
    "    ### Load a result\n",
    "    if os.path.isfile('hyperparameter_searches/' + file_name + '.json'):\n",
    "        with open('hyperparameter_searches/' + file_name + '.json', 'r') as ff:\n",
    "            old_results_list = json.load(ff)\n",
    "        ff.close()\n",
    "        results_list = old_results_list.copy()\n",
    "    else:\n",
    "        old_results_list = []\n",
    "        results_list = [parameter_dictionary]\n",
    "else:\n",
    "    results_list = [parameter_dictionary]\n",
    "\n",
    "with open('hyperparameter_searches/progress.txt', 'w+') as f:\n",
    "    \n",
    "    date_string_proper = datetime.datetime.now().strftime(\"%H:%M, %Y-%m-%d\")\n",
    "    f.write('Benchmark done on input parameters at ' + date_string_proper + '\\n\\n')\n",
    "    f.flush()\n",
    "    \n",
    "    # load the selected galaxyfile, be sure that the autoloaded galaxyfile is the one that you want to use!!\n",
    "    galaxies, data_keys = load_galfiles(redshifts=redshifts, equal_numbers=same_n_points_per_redshift)\n",
    "\n",
    "    for i_act_fun, act_fun in enumerate(activation_functions):\n",
    "        for i_reg_strength, reg_strength in enumerate(reg_strengths):\n",
    "            date_string_proper = datetime.datetime.now().strftime(\"%H:%M, %Y-%m-%d\")\n",
    "            f.write(date_string_proper + '  Starting new constant unit\\n\\n')\n",
    "            f.flush()\n",
    "            for i_neur_per_lay, neur_per_lay in enumerate(neurons_per_layer):\n",
    "#                 for i_nr_lay, nr_lay in enumerate(nr_layers):\n",
    "                for i_batch_size, batch_size in enumerate(batch_sizes):\n",
    "\n",
    "                    ### Network parameters\n",
    "                    network_args = {        \n",
    "                        'nr_hidden_layers': nr_layers[i_neur_per_lay],\n",
    "                        'nr_neurons_per_lay': neur_per_lay,\n",
    "                        'input_features': input_features,\n",
    "                        'output_features': output_features,\n",
    "                        'activation_function': act_fun,\n",
    "                        'output_activation': output_activation,\n",
    "                        'reg_strength': reg_strength\n",
    "                    }\n",
    "\n",
    "                    earlystop = EarlyStopping(monitor='val_loss', min_delta=early_stop_min_delta, \n",
    "                                              patience=early_stop_patience[i_batch_size], verbose=1, mode='auto')\n",
    "                    nan_termination = TerminateOnNaN()\n",
    "                    callbacks_list = [earlystop, nan_termination]\n",
    "\n",
    "                    train_histories = []\n",
    "                    val_histories = []\n",
    "                    scores = []\n",
    "                    \n",
    "                    if keep_writing_to_old_file:\n",
    "                        comb_tried = False\n",
    "                        for params in [item[0] for item in old_results_list[1:]]:\n",
    "                            if params['nr_lay'] == network_args['nr_hidden_layers']:\n",
    "                                comb_tried = True\n",
    "                                run_counter += nr_folds\n",
    "                                \n",
    "                    if (not comb_tried) or (not keep_writing_to_old_file):\n",
    "                                \n",
    "                        for fold in range(nr_folds):\n",
    "\n",
    "                            date_string_proper = datetime.datetime.now().strftime(\"%H:%M, %Y-%m-%d\")\n",
    "                            run_counter += 1\n",
    "                            f.write(date_string_proper + '        Run nr %d/%d. \\n\\n' % (run_counter, tot_nr_runs))\n",
    "                            f.flush()\n",
    "\n",
    "                            ### Prepare the training data\n",
    "\n",
    "                            # prepare the training data\n",
    "                            training_data_dict = divide_train_data(galaxies, data_keys, network_args, redshifts, \n",
    "                                                                   outputs_to_weigh=outputs_to_weigh, \n",
    "                                                                   weigh_by_redshift=weigh_by_redshift, total_set_size=tot_nr_points, \n",
    "                                                                   train_frac=train_frac, val_frac=val_frac, \n",
    "                                                                   test_frac=test_frac, emerge_targets=True)\n",
    "                            # galaxies = None\n",
    "                            training_data_dict = normalise_data(training_data_dict, norm)\n",
    "\n",
    "                            model = standard_network(network_args['input_features'], network_args['output_features'], \n",
    "                                                     network_args['nr_neurons_per_lay'], network_args['nr_hidden_layers'], \n",
    "                                                     network_args['activation_function'], \n",
    "                                                     network_args['output_activation'], network_args['reg_strength'], clipvalue=.001)\n",
    "\n",
    "                            # Fit the model                        \n",
    "                            history = model.fit(x = training_data_dict['input_train_dict'], y = training_data_dict['output_train_dict'], \n",
    "                                                validation_data = (training_data_dict['input_'+validation_data+'_dict'], \n",
    "                                                training_data_dict['output_'+validation_data+'_dict'], training_data_dict['val_weights']), \n",
    "                                                epochs=int(nr_epochs[i_batch_size]), batch_size=int(batch_size), callbacks=callbacks_list,\n",
    "                                                sample_weight=training_data_dict['train_weights'], verbose=verb)\n",
    "\n",
    "                            test_scores = model.evaluate(x=training_data_dict['input_test_dict'], y=training_data_dict['output_test_dict'],\n",
    "                                                         sample_weight=training_data_dict['test_weights'], verbose=verb)\n",
    "                            scores.append(test_scores[0]) # take total test result\n",
    "\n",
    "                            if 'loss' in history.history:\n",
    "                                train_histories.append(history.history['loss'])\n",
    "                            if 'val_loss' in history.history:                        \n",
    "                                val_histories.append(history.history['val_loss'])\n",
    "\n",
    "                        best_score = np.amin(scores)\n",
    "                        mean_score = np.mean(scores)\n",
    "                        score_std = np.std(scores)\n",
    "                        total_score = [mean_score, score_std, best_score]\n",
    "                        parameters = {'nr_lay': nr_layers[i_neur_per_lay], 'neur_per_lay': neur_per_lay, \n",
    "                                     'act_fun': act_fun, 'batch_size': batch_size, 'reg_strength': reg_strength}\n",
    "                        results_list.append([parameters, total_score, train_histories, val_histories])\n",
    "                        \n",
    "                        with open('hyperparameter_searches/' + file_name + '.json', 'w+') as ff:\n",
    "                            json.dump(results_list, ff)\n",
    "                        ff.close()\n",
    "            \n",
    "    date_string_proper = datetime.datetime.now().strftime(\"%H:%M, %Y-%m-%d\")\n",
    "    f.write('Benchmark completed at ' + date_string_proper + '\\n')\n",
    "    f.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save the result\n",
    "date_string = datetime.datetime.now().strftime('%Y-%m-%d--%H-%M-%S')\n",
    "custom_string = 'network_size'\n",
    "tot_string = date_string + '_' + custom_string + '.json'\n",
    "with open('hyperparameter_searches/' + tot_string, 'w+') as f:\n",
    "    json.dump(results_list, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load a result\n",
    "loaded_list_string = '2018-09-24_network_size'\n",
    "with open('hyperparameter_searches/' + loaded_list_string + '.json', 'r') as f:\n",
    "    results_list = json.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(results_list[1][1])\n",
    "print(results_list[2][1])\n",
    "print(results_list[3][1])\n",
    "print(results_list[4][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Plot the loss histories to make sure that the best performance was reached\n",
    "contains_data_list = [True if lst[2] else False for lst in results_list[1:]]\n",
    "print('Number of parameter combinations that ended up exploding before test loss could be evaluated: {:d}'.format(\n",
    "      np.sum(not contains_data_list)))\n",
    "print('Number of parameter combinations that contains at least one test loss data point: {:d}'.format(\n",
    "      np.sum(contains_data_list)))\n",
    "for comb_nr, lst in enumerate(results_list[1:]):\n",
    "    \n",
    "    if lst[2]:\n",
    "        title = str(lst[0]) + '\\n mean final test score: {:.2e}'.format(lst[1][0])\n",
    "        fig = plt.figure(5, figsize=(12,12))\n",
    "        for i in range(len(lst[2])):\n",
    "            train_losses = lst[2][i]\n",
    "            val_losses = lst[3][i]\n",
    "            plt.plot(range(1, len(train_losses)+1), train_losses, 'b')\n",
    "            plt.plot(range(1, len(val_losses)+1), val_losses, 'r')\n",
    "        plt.yscale('log')\n",
    "        plt.title(title)\n",
    "        plt.ylabel('loss')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'val'], loc='upper right')\n",
    "        plt.show()\n",
    "  #  else:\n",
    "  #      print('No data here, boss')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot over network size importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unit_dict = get_unit_dict()\n",
    "'$log_{{10}}([{}])$'.format(unit_dict['SSFR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masses_lin_vals = np.linspace(0, 10, num=100)\n",
    "scale_factors_lin_vals = np.linspace(20, 25, num=100)\n",
    "masses_grid_vals, scale_factors_grid_vals = np.meshgrid(masses_lin_vals, scale_factors_lin_vals)\n",
    "print(masses_lin_vals)\n",
    "print(masses_grid_vals)\n",
    "print(scale_factors_grid_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_color = 'xkcd:blue'\n",
    "size_color = 'xkcd:orange'\n",
    "\n",
    "fig99999 = plt.figure(figsize=(12,6))\n",
    "ax1 = plt.subplot(111)\n",
    "net_sizes = [lst[0]['nr_lay'] for lst in results_list[1:]]\n",
    "order_indeces = np.argsort(net_sizes)\n",
    "mean_test_scores = [lst[1][0] for lst in results_list[1:]]\n",
    "stds = [lst[1][1] for lst in results_list[1:]]\n",
    "ax1.errorbar(np.array(net_sizes)[order_indeces], np.array(mean_test_scores)[order_indeces], yerr=np.array(stds)[order_indeces], fmt='o-', markersize=3, capsize=5, color=loss_color)\n",
    "ax1.set_xlabel('Network size', fontsize=25)\n",
    "ax1.set_ylabel('Mean mse loss $\\pm 1\\sigma$', fontsize=25, color=loss_color)\n",
    "ax1.set_xticks(np.array(net_sizes)[order_indeces])\n",
    "ax1.tick_params(labelsize=20)\n",
    "ax1.tick_params(axis='y', colors=loss_color)\n",
    "ax1.ticklabel_format(axis='y', style='sci', scilimits=(-2, 2))\n",
    "ax1.yaxis.get_offset_text().set_fontsize(25)\n",
    "ax1.set_xlim(left=1.5)\n",
    "\n",
    "nr_variables = []\n",
    "for net_size in np.array(net_sizes)[order_indeces]:\n",
    "    nr_var, weight_shapes = standard_network_get_nr_variables_weight_shapes(['tmp']*5, ['tmp']*2, net_size, net_size)\n",
    "    nr_variables.append(nr_var)\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "ax2.plot(np.array(net_sizes)[order_indeces], nr_variables, 'o', color=size_color)\n",
    "ax2.set_ylabel('Nr network variables', fontsize=25, color=size_color)\n",
    "ax2.tick_params(axis='y', labelsize=20, colors=size_color)\n",
    "ax2.semilogy()\n",
    "for i, (x,y) in enumerate(zip(np.array(net_sizes)[order_indeces], nr_variables)):\n",
    "    if i == 0:\n",
    "        ax2.annotate('{:>4}'.format(str(y)),xy=(x-1,y*.9), fontsize=15, color=size_color)\n",
    "    elif i <= 5:\n",
    "        ax2.annotate('{:>4}'.format(str(y)),xy=(x-1.1,y*.9), fontsize=15, color=size_color)\n",
    "    elif i <= 8:\n",
    "        ax2.annotate('{:>4}'.format(str(y)),xy=(x-1.2,y*.9), fontsize=15, color=size_color)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig99999.savefig('hyperparameter_searches/' + loaded_list_string + '_error_plot.png', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Old code again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model TESTING\n",
    "inputs = []\n",
    "\n",
    "main_input = Input(shape=(len(input_features),), name = 'main_input')\n",
    "halo_mass_input = Input(shape=(1,), name = 'Halo_mass')\n",
    "sfr_input = Input(shape=(1,), name = 'SFR')\n",
    "stellar_mass_input = Input(shape=(1,), name = 'Stellar_mass')\n",
    "inputs.append(main_input)\n",
    "inputs.append(halo_mass_input)\n",
    "#inputs.append(sfr_input)\n",
    "#inputs.append(stellar_mass_input)\n",
    "#for i_feat, feat in enumerate(weighted_output_features):\n",
    "#    inputs.append(Input(shape=(1,), name = feat))\n",
    "    \n",
    "for i in range(0, nLayers-1): # -1 because one layer is added automatically with the input layer\n",
    "    if i == 0:\n",
    "        #x = concatenate([halo_mass_input, others_input])\n",
    "        #x = Dense(neuronsPerLayer, activation = activationFunction)(main_input)\n",
    "        x = Dense(neuronsPerLayer, kernel_regularizer=regularizers.l2(reg_strength))(main_input)\n",
    "        x = LeakyReLU(alpha = 0.1)(x)\n",
    "    else:\n",
    "        #x = Dense(neuronsPerLayer, activation = activationFunction)(x)\n",
    "        x = Dense(neuronsPerLayer, kernel_regularizer=regularizers.l2(reg_strength))(x)\n",
    "        x = LeakyReLU(alpha = 0.1)(x)\n",
    "        \n",
    "outputs = []\n",
    "\n",
    "sfr_output = Dense(1, kernel_regularizer=regularizers.l2(reg_strength))(x)\n",
    "sfr_weigh_loss = Nonweighted_loss_layer()([sfr_input, sfr_output])\n",
    "\n",
    "stellar_mass_output = Dense(1, kernel_regularizer=regularizers.l2(reg_strength))(x)\n",
    "stellar_mass_weigh_loss = Weighted_loss_layer()([halo_mass_input, stellar_mass_input, \n",
    "                                                                    stellar_mass_output])\n",
    "\n",
    "outputs.append(sfr_weigh_loss)\n",
    "outputs.append(stellar_mass_weigh_loss)\n",
    "\n",
    "#out = Weighted_loss_layer()([halo_mass_input, ])\n",
    "\n",
    "model = Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "model_core.compile(optimizer = 'adam', loss = tunnel_loss)\n",
    "\n",
    "earlystop = EarlyStopping(monitor=early_stop_monitor, min_delta=early_stop_min_delta, patience=early_stop_patience, \\\n",
    "                          verbose=1, mode='auto')\n",
    "callbacks_list = [earlystop]\n",
    "\n",
    "history = model_core.fit(x = training_data_dict['input_train_dict'], y = training_data_dict['output_train_dict'], \n",
    "                    validation_data = (training_data_dict['input_'+validation_data+'_dict'], \n",
    "                    training_data_dict['output_'+validation_data+'_dict']), \n",
    "                    epochs=int(nr_epochs), batch_size=int(batch_size), callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = model.layers[5].get_weights()\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = '6x6_all-points_redshifts00-01-02-05-10-20-30-40-60-80_tanh_Halo_mass-Halo_mass_peak-Scale_peak_mass-Scale_half_mass-Halo_growth_rate-Redshift_to_Stellar_mass-SFR_test_score7.40e-07'\n",
    "model = load_model(bp_network_dir + model_name + '/model.h5')\n",
    "model_weights = model.get_weights()\n",
    "\n",
    "position = []\n",
    "for weight_matrix in model_weights:\n",
    "    position.append(np.ndarray.flatten(weight_matrix))\n",
    "\n",
    "pickle.dump(position, open(bp_network_dir + model_name + '/best_position.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Search for the model that you want\n",
    "importlib.reload(model_management)\n",
    "search_dict = {\n",
    "    'training_method': 'backprop'\n",
    "}\n",
    "[model_dicts, description_dicts] = model_management.SearchModel(search_dict, get_hits=True)\n",
    "print(description_dicts)\n",
    "print('\\n')\n",
    "for key in model_dicts:\n",
    "    print(key)\n",
    "    print(model_dicts[key])\n",
    "    print('\\n')\n",
    "\n",
    "\n",
    "importlib.reload(model_management)\n",
    "model, model_dict, description = model_management.LoadModel(search_dict, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figggg = plt.figure(figsize=(8,8))\n",
    "ax = plt.subplot(1, 1, 1)\n",
    "ax.plot(np.arange(0,10)+1, np.sin(np.arange(0,10)+1))\n",
    "ax.set_xscale('log')\n",
    "ax.set_xticks(np.arange(0,10)+1)\n",
    "ax.set_xticklabels(np.arange(0,10))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
